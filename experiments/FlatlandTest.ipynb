{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FlatlandTest.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/partlygloudy/joint-attention/blob/master/experiments/FlatlandTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq1Ws7n4zwYb",
        "colab_type": "code",
        "outputId": "4f272fe2-dc4e-4933-b9f2-16815b064aa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Install flatland tasks and learning agent architectures from GitHub repo\n",
        "!git clone https://github.com/partlygloudy/joint-attention.git \"joint_attention\"\n",
        "\n",
        "# Install pygame\n",
        "!pip install pygame\n",
        "\n",
        "# Configure to use preferred tensorflow version\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "# Import other requirements\n",
        "import time\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'joint_attention' already exists and is not an empty directory.\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.6/dist-packages (1.9.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nKjBfsVgv93",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "71301fea-1c74-4057-a476-876c172c5368"
      },
      "source": [
        "from joint_attention.flatland import flatland_tasks\n",
        "from joint_attention.learning_agents import dqn\n",
        "import importlib"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pygame 1.9.6\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36v_UpNcmDyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -- EXPERIMENT PARAMETERS -- #\n",
        "mem_capacity = 250000            # Replay memory size\n",
        "training_frames = 2500000         # Total steps to train over\n",
        "epoch_size = 5000                # Number of steps in a single epoch\n",
        "e = 1.0                          # Starting value for epsilon\n",
        "e_min = 0.05                     # Final value for epsilon\n",
        "e_steps = 150000                 # Number of steps epsilon decreases over\n",
        "discount = 0.99\n",
        "batch_size = 32\n",
        "k = 1                            # Number of steps between training batches\n",
        "trials = 1                       # Number of times to run the whole experiment\n",
        "random_buffer_len = 5000        # Number of initial random actions to take"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EE49dFHvnf-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -- DATA COLLECTION -- #\n",
        "\n",
        "reward_last_10 = [0.0] * 10\n",
        "eval_hist = []\n",
        "training_eval_hist = []\n",
        "e_hist = []\n",
        "epoch_counter = 1\n",
        "game_counter = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTSKefKs0YTg",
        "colab_type": "code",
        "outputId": "358edcae-d052-4973-9718-831e3f06019d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# importlib.reload(dqn)\n",
        "# importlib.reload(flatland_tasks)\n",
        "\n",
        "# -- TRAINING LOOP -- #\n",
        "\n",
        "# Create environment\n",
        "env = flatland_tasks.TaskFood200()\n",
        "\n",
        "# Initialize agent\n",
        "agent = dqn.DQNAgent(memsize=mem_capacity, gamma=discount)\n",
        "\n",
        "# Train agent\n",
        "current_frame = 0\n",
        "while current_frame < training_frames:\n",
        "    \n",
        "    # Reset the game\n",
        "    game_reward = 0.0\n",
        "    s = env.reset()\n",
        "\n",
        "    # Run until game ends\n",
        "    done = False\n",
        "    while not done:\n",
        "\n",
        "        # Choose randomly with probability e\n",
        "        if current_frame >= random_buffer_len:\n",
        "            e = max(e_min, e - ((1.0 - e_min) / e_steps))\n",
        "        if random.random() < e or current_frame < random_buffer_len:\n",
        "            a = random.randint(0, 3)\n",
        "\n",
        "        # Otherwise choose best action as predicted by agent\n",
        "        else:\n",
        "            a = agent.choose_action(s)\n",
        "\n",
        "        # Take the action\n",
        "        if a == 1:\n",
        "            a = 0\n",
        "        s_prime, r, done = env.step(a)\n",
        "        \n",
        "        # Add experience to memory\n",
        "        agent.memory.add(s, a, r, s_prime)\n",
        "        \n",
        "        # Update network every k timesteps\n",
        "        if agent.memory.size >= random_buffer_len and current_frame % k == 0:\n",
        "            agent.learn_from_memory(batch_size)\n",
        "            \n",
        "        # Update current state\n",
        "        s = s_prime\n",
        "        \n",
        "        # Update tracking info\n",
        "        game_reward += r\n",
        "        current_frame += 1\n",
        "\n",
        "    # Game finished, log data\n",
        "    reward_last_10[game_counter % 10] = game_reward\n",
        "    game_counter += 1\n",
        "        \n",
        "    if current_frame >= epoch_counter * epoch_size:\n",
        "\n",
        "        # Print epoch results\n",
        "        out_str = \"Epoch #\" + str(epoch_counter)\n",
        "        out_str += \"\\tTotal Games:\" + str(game_counter)\n",
        "        out_str += \"\\tReward avg: \" + str(sum(reward_last_10) / 10.0)\n",
        "        if len(eval_hist) > 0:\n",
        "            out_str += \"\\tEval. Avg.: \" + str(eval_hist[-1])\n",
        "        out_str += \"\\te: \" + str(e)\n",
        "                                    \n",
        "        print(out_str)\n",
        "            \n",
        "        # Increment epoch counter\n",
        "        epoch_counter += 1 \n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Epoch #1\tTotal Games:24\tReward avg: 2.3\te: 0.9996833333333335\n",
            "Epoch #2\tTotal Games:52\tReward avg: 1.4\te: 0.9680166666666814\n",
            "Epoch #3\tTotal Games:75\tReward avg: 2.4\te: 0.9344500000000302\n",
            "Epoch #4\tTotal Games:101\tReward avg: 1.9\te: 0.9043666666667107\n",
            "Epoch #5\tTotal Games:126\tReward avg: 2.3\te: 0.8727000000000587\n",
            "Epoch #6\tTotal Games:149\tReward avg: 2.8\te: 0.8416666666667396\n",
            "Epoch #7\tTotal Games:173\tReward avg: 2.4\te: 0.809050000000088\n",
            "Epoch #8\tTotal Games:197\tReward avg: 1.6\te: 0.7780166666667689\n",
            "Epoch #9\tTotal Games:221\tReward avg: 2.9\te: 0.7466666666667834\n",
            "Epoch #10\tTotal Games:241\tReward avg: 3.3\te: 0.7143666666667983\n",
            "Epoch #11\tTotal Games:262\tReward avg: 3.0\te: 0.6823833333334797\n",
            "Epoch #12\tTotal Games:285\tReward avg: 2.8\te: 0.6507166666668276\n",
            "Epoch #13\tTotal Games:310\tReward avg: 1.8\te: 0.6200000000001751\n",
            "Epoch #14\tTotal Games:337\tReward avg: 2.1\te: 0.5873833333335234\n",
            "Epoch #15\tTotal Games:361\tReward avg: 2.2\te: 0.5566666666668709\n",
            "Epoch #16\tTotal Games:385\tReward avg: 2.0\te: 0.5250000000002188\n",
            "Epoch #17\tTotal Games:408\tReward avg: 2.4\te: 0.49301666666690025\n",
            "Epoch #18\tTotal Games:432\tReward avg: 2.2\te: 0.4616666666669147\n",
            "Epoch #19\tTotal Games:455\tReward avg: 3.1\te: 0.4293666666669296\n",
            "Epoch #20\tTotal Games:477\tReward avg: 2.5\te: 0.397383333333611\n",
            "Epoch #21\tTotal Games:500\tReward avg: 1.7\te: 0.36635000000029194\n",
            "Epoch #22\tTotal Games:523\tReward avg: 2.5\te: 0.33310000000030726\n",
            "Epoch #23\tTotal Games:549\tReward avg: 2.1\te: 0.3030166666669878\n",
            "Epoch #24\tTotal Games:569\tReward avg: 3.0\te: 0.2710333333336692\n",
            "Epoch #25\tTotal Games:591\tReward avg: 2.4\te: 0.24000000000035016\n",
            "Epoch #26\tTotal Games:614\tReward avg: 2.3\te: 0.20770000000036504\n",
            "Epoch #27\tTotal Games:636\tReward avg: 2.5\te: 0.17540000000037992\n",
            "Epoch #28\tTotal Games:658\tReward avg: 3.2\te: 0.14436666666706088\n",
            "Epoch #29\tTotal Games:680\tReward avg: 2.0\te: 0.1127000000004088\n",
            "Epoch #30\tTotal Games:700\tReward avg: 3.5\te: 0.08040000000042369\n",
            "Epoch #31\tTotal Games:716\tReward avg: 3.8\te: 0.05\n",
            "Epoch #32\tTotal Games:733\tReward avg: 4.6\te: 0.05\n",
            "Epoch #33\tTotal Games:753\tReward avg: 3.7\te: 0.05\n",
            "Epoch #34\tTotal Games:772\tReward avg: 3.8\te: 0.05\n",
            "Epoch #35\tTotal Games:793\tReward avg: 2.5\te: 0.05\n",
            "Epoch #36\tTotal Games:811\tReward avg: 3.8\te: 0.05\n",
            "Epoch #37\tTotal Games:832\tReward avg: 3.6\te: 0.05\n",
            "Epoch #38\tTotal Games:852\tReward avg: 3.0\te: 0.05\n",
            "Epoch #39\tTotal Games:871\tReward avg: 4.5\te: 0.05\n",
            "Epoch #40\tTotal Games:890\tReward avg: 2.3\te: 0.05\n",
            "Epoch #41\tTotal Games:909\tReward avg: 4.8\te: 0.05\n",
            "Epoch #42\tTotal Games:927\tReward avg: 4.0\te: 0.05\n",
            "Epoch #43\tTotal Games:945\tReward avg: 3.3\te: 0.05\n",
            "Epoch #44\tTotal Games:966\tReward avg: 1.9\te: 0.05\n",
            "Epoch #45\tTotal Games:984\tReward avg: 4.7\te: 0.05\n",
            "Epoch #46\tTotal Games:1005\tReward avg: 3.0\te: 0.05\n",
            "Epoch #47\tTotal Games:1026\tReward avg: 2.1\te: 0.05\n",
            "Epoch #48\tTotal Games:1048\tReward avg: 2.3\te: 0.05\n",
            "Epoch #49\tTotal Games:1069\tReward avg: 2.7\te: 0.05\n",
            "Epoch #50\tTotal Games:1089\tReward avg: 3.6\te: 0.05\n",
            "Epoch #51\tTotal Games:1110\tReward avg: 3.4\te: 0.05\n",
            "Epoch #52\tTotal Games:1132\tReward avg: 3.4\te: 0.05\n",
            "Epoch #53\tTotal Games:1148\tReward avg: 3.8\te: 0.05\n",
            "Epoch #54\tTotal Games:1165\tReward avg: 3.9\te: 0.05\n",
            "Epoch #55\tTotal Games:1186\tReward avg: 3.4\te: 0.05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-4f216241e477>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Otherwise choose best action as predicted by agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Take the action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/joint_attention/learning_agents/dqn.py\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;31m# Return the best action for a given state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m         callbacks=callbacks)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;31m# Setup work for each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_training_eval_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m       \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m     \u001b[0;31m# Reset metrics on all the distributed (cloned) models.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/metrics.py\u001b[0m in \u001b[0;36mreset_states\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0mwhen\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mevaluated\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \"\"\"\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   3257\u001b[0m           \u001b[0massign_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3258\u001b[0m           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3259\u001b[0;31m         \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mC5hiT53Wxb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "0f5dee64-e22b-43cb-de4a-f36406030986"
      },
      "source": [
        "# ### TRIAL VIDEO CAPTURE STUFF ###\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Create directory for storing frames and video\n",
        "# os.mkdir(\"video\")\n",
        "\n",
        "# Number of sample videos to record\n",
        "trials = 5\n",
        "\n",
        "for t in range(1, trials + 1):\n",
        "\n",
        "    # Create instance of environment for trial\n",
        "    env = flatland_tasks.TaskFood200()\n",
        "    s = env.reset()\n",
        "    sample_frame = env.render(mode=\"return\")\n",
        "    dims = (sample_frame.shape[1], sample_frame.shape[0])\n",
        "\n",
        "    # Video writer object\n",
        "    vid = cv2.VideoWriter('video/test_vid_' + str(t) + '.avi', cv2.VideoWriter_fourcc(*'DIVX'), 10, dims)\n",
        "\n",
        "    # Frame counter\n",
        "    frames = 0\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "        \n",
        "        a = agent.choose_action(s)\n",
        "        s_prime, r, done = env.step(a)\n",
        "        s = s_prime\n",
        "\n",
        "        # Save rendered game state\n",
        "        vid.write(env.render(mode=\"return\"))          \n",
        "        frames += 1\n",
        "    \n",
        "    # Print info\n",
        "    print(\"Trial #\" + str(t) + \" recorded\\t-\\t\" + str(frames) + \" frames captured\")\n",
        "\n",
        "    # Release VideoWriter object\n",
        "    vid.release()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial #1 recorded\t-\t250 frames captured\n",
            "Trial #2 recorded\t-\t100 frames captured\n",
            "Trial #3 recorded\t-\t250 frames captured\n",
            "Trial #4 recorded\t-\t350 frames captured\n",
            "Trial #5 recorded\t-\t200 frames captured\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g53Lcu9iEXfx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}